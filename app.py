# ==============================================================================
# GEREKLÄ° KÃœTÃœPHANELER (app.py)
# AmaÃ§: Streamlit web arayÃ¼zÃ¼ ve RAG zinciri iÃ§in gerekli tÃ¼m
#       Python kÃ¼tÃ¼phanelerini iÃ§e aktarmak.
# ==============================================================================
import streamlit as st  # Web arayÃ¼zÃ¼nÃ¼ (butonlar, metin kutularÄ± vb.) oluÅŸturmak iÃ§in
import os               # Ä°ÅŸletim sistemiyle etkileÅŸim iÃ§in (API anahtarÄ±nÄ± ayarlamak, dosya yolunu kontrol etmek)
import pandas as pd       # (Bu kodda doÄŸrudan kullanÄ±lmÄ±yor ama genellikle veri gÃ¶stermek iÃ§in eklenir)
from tqdm import tqdm     # (Bu kodda doÄŸrudan kullanÄ±lmÄ±yor, Colab'de kullanÄ±lmÄ±ÅŸtÄ±)

# --- LangChain KÃ¼tÃ¼phaneleri ---
# Colab notebook'unda RAG zincirini kurarken kullandÄ±ÄŸÄ±mÄ±z
# tÃ¼m temel bileÅŸenleri buraya da import ediyoruz.
from langchain_text_splitters import RecursiveCharacterTextSplitter # (Bu dosyada gerek yok ama zararÄ± da yok)
from langchain_core.documents import Document                     # (Bu dosyada gerek yok ama zararÄ± da yok)
from langchain_chroma import Chroma                     # Diskten 'chroma_db' veritabanÄ±nÄ± yÃ¼klemek iÃ§in
from langchain_huggingface import HuggingFaceEmbeddings # VeritabanÄ±nÄ± yÃ¼klerken hangi embedding modelini kullandÄ±ÄŸÄ±mÄ±zÄ± belirtmek iÃ§in
from langchain_google_genai import ChatGoogleGenerativeAI # RAG zincirinin beyni olan LLM'i (Gemini) yÃ¼klemek iÃ§in
from langchain_core.prompts import PromptTemplate       # LLM'e vereceÄŸimiz talimat ÅŸablonu (Prompt) iÃ§in
from langchain_core.runnables import RunnablePassthrough # Zincir iÃ§inde kullanÄ±cÄ± sorusunu deÄŸiÅŸtirmeden aktarmak iÃ§in
from langchain_core.output_parsers import StrOutputParser # LLM'in cevabÄ±nÄ± temiz bir metne (string) dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in
# ==============================================================================
# GOOGLE API ANAHTARI AYARI (LOKAL/DEPLOYMENT Ä°Ã‡Ä°N DÃœZELTÄ°LDÄ°)
# ==============================================================================

# Bu fonksiyon, UYGULAMANIN NEREDE Ã‡ALIÅTIÄINI KONTROL EDER.
# Streamlit Cloud'daysa, 'st.secrets'tan anahtarÄ± okur.
# Lokal (senin bilgisayarÄ±n) ise, kenar Ã§ubuÄŸunda metin kutusunu gÃ¶sterir.

def get_google_api_key():
    # 'os.environ.get' ile ortam deÄŸiÅŸkenlerini kontrol ediyoruz.
    # "STREAMLIT_SHARING_MODE" deÄŸiÅŸkeni SADECE Streamlit Cloud'da "True" olur.
    # Bu kontrol, st.secrets'Ä± lokalde aramayÄ± engeller ve Ã§Ã¶kmeyi Ã¶nler.
    if os.environ.get('STREAMLIT_SHARING_MODE') == 'True':
        # Streamlit Cloud'da Ã§alÄ±ÅŸÄ±yoruz, Secrets'tan anahtarÄ± al
        try:
            return st.secrets["GOOGLE_API_KEY"]
        except KeyError:
            # Secrets'ta anahtarÄ±n unutulmasÄ± durumuna karÅŸÄ± bir hata
            st.error("HATA: Streamlit Cloud Secrets'ta 'GOOGLE_API_KEY' bulunamadÄ±!")
            st.stop()
    else:
        # Lokal'de Ã§alÄ±ÅŸÄ±yoruz, kenar Ã§ubuÄŸundan anahtarÄ± iste
        return st.sidebar.text_input(
            "Google API AnahtarÄ±nÄ±zÄ± Buraya YapÄ±ÅŸtÄ±rÄ±n:",
            type="password",
            help="API anahtarÄ±nÄ±zÄ± Google AI Studio'dan alabilirsiniz."
        )

# Fonksiyonu Ã§aÄŸÄ±r ve anahtarÄ± al
api_key = get_google_api_key()

# AnahtarÄ±n alÄ±nÄ±p alÄ±nmadÄ±ÄŸÄ±nÄ± kontrol et
if api_key:
    # AnahtarÄ±, LangChain kÃ¼tÃ¼phanelerinin otomatik olarak okuyacaÄŸÄ±
    # 'os.environ' (ortam deÄŸiÅŸkeni) iÃ§ine ata.
    os.environ["GOOGLE_API_KEY"] = api_key
else:
    # EÄŸer kullanÄ±cÄ± henÃ¼z bir anahtar girmemiÅŸse (ve Secrets'ta da yoksa):
    # Sol kenar Ã§ubuÄŸuna bir uyarÄ± koy
    st.sidebar.warning("LÃ¼tfen sol kenar Ã§ubuÄŸundan Google API anahtarlarÄ±nÄ±zÄ± girin.")
    # AnahtarÄ± girmeden uygulamanÄ±n geri kalanÄ±nÄ±n Ã§alÄ±ÅŸmasÄ±nÄ± durdur
    st.stop()
# ==============================================================================
# RAG ZÄ°NCÄ°RÄ°NÄ° YÃœKLEME FONKSÄ°YONU (HÃ¼cre 4'Ã¼n GeliÅŸmiÅŸ Hali)
# AmaÃ§: RAG zincirini (Retriever, Prompt, LLM) kuran ana fonksiyon.
# ==============================================================================

# @st.cache_resource: Bu, Streamlit iÃ§in HAYATÄ° bir optimizasyon komutudur.
# Bu 'decorator', altÄ±ndaki 'load_rag_chain' fonksiyonunun SADECE BÄ°R KEZ
# Ã§alÄ±ÅŸmasÄ±nÄ± ve sonucunu (kurulan 'rag_chain') hafÄ±zada (cache) tutmasÄ±nÄ± saÄŸlar.
#
# EÄER BU OLMASAYDI: KullanÄ±cÄ± her soru sorduÄŸunda, Streamlit tÃ¼m 'app.py'
# dosyasÄ±nÄ± baÅŸtan Ã§alÄ±ÅŸtÄ±rÄ±r ve her seferinde modeli + veritabanÄ±nÄ±
# yeniden yÃ¼klerdi. Bu da saniyeler sÃ¼ren yavaÅŸ bir uygulama demekti.
# @st.cache_resource sayesinde model ve DB bir kez yÃ¼klenir, anÄ±nda cevap alÄ±nÄ±r.
@st.cache_resource
def load_rag_chain():
    """
    (Fonksiyon aÃ§Ä±klamasÄ± - Docstring)
    Colab'de oluÅŸturulan 'chroma_db' veritabanÄ±nÄ± diskten yÃ¼kler
    ve RAG zincirini kurar.
    """
    # KullanÄ±cÄ±ya modelin yÃ¼klendiÄŸini bildiren bir durum mesajÄ±
    st.sidebar.info("RAG Zinciri ve Model YÃ¼kleniyor...")
    
    # --- 1. Retriever'Ä± Kur (VeritabanÄ± YÃ¼kleme) ---
    # Colab'de 'chroma_db'yi oluÅŸtururken kullandÄ±ÄŸÄ±mÄ±z modelin aynÄ±sÄ± olmalÄ±.
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    
    # 'app.py'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ± dizinde 'chroma_db' adÄ±nda bir klasÃ¶r ara.
    if not os.path.exists("chroma_db"):
        # EÄŸer klasÃ¶r bulunamazsa:
        # Ana ekrana bÃ¼yÃ¼k bir HATA mesajÄ± bas.
        st.error(
            "HATA: 'chroma_db' klasÃ¶rÃ¼ bulunamadÄ±! "
            "LÃ¼tfen Colab'den indirdiÄŸiniz 'chroma_db' klasÃ¶rÃ¼nÃ¼ 'app.py' dosyasÄ±nÄ±n yanÄ±na kopyalayÄ±n."
        )
        st.stop() # UygulamayÄ± durdur
        
    # 'chroma_db' klasÃ¶rÃ¼nÃ¼ diskten yÃ¼kle ve 'vectorstore' nesnesini oluÅŸtur.
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embeddings)
    
    # Optimize ettiÄŸimiz en iyi ayar (Colab HÃ¼cre 4'teki gibi):
    # KullanÄ±cÄ±nÄ±n sorusuna en Ã§ok benzeyen 'k=1' (en iyi 1) sonucu getir.
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1}) 
    
    # --- 2. Prompt'u Kur (Colab HÃ¼cre 4'teki gibi) ---
    # LLM'e (Gemini) vereceÄŸimiz talimat ÅŸablonu.
    template = """
    Sana verilen BaÄŸlam'Ä± kullanarak kullanÄ±cÄ±nÄ±n Soru'sunu cevapla. CevabÄ±nÄ± SADECE bu baÄŸlamdaki bilgilere dayanarak ver. EÄŸer baÄŸlamda cevap yoksa, 'Bu konuda bilgim yok.' de.
    BaÄŸlam:
    {context}
    Soru: {question}
    Cevap:
    """
    prompt = PromptTemplate.from_template(template)

    # --- 3. LLM'i Kur (Colab HÃ¼cre 4'teki gibi) ---
    # 'gemini-pro-latest' gÃ¼nlÃ¼k 50 limitine takÄ±labilir.
    llm = ChatGoogleGenerativeAI(model="gemini-pro-latest", temperature=0)

    # --- 4. Zinciri (Chain) OluÅŸtur (Colab HÃ¼cre 4'teki gibi) ---
    # LangChain Expression Language (LCEL) kullanarak boru hattÄ±nÄ± tanÄ±mla.
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}  # AdÄ±m 1: Context'i bul, soruyu aktar
        | prompt                                                   # AdÄ±m 2: Prompt'u doldur
        | llm                                                      # AdÄ±m 3: LLM'e sor
        | StrOutputParser()                                        # AdÄ±m 4: CevabÄ± metne Ã§evir
    )
    
    # Model yÃ¼klemesi bittiÄŸinde kenar Ã§ubuÄŸundaki 'info' mesajÄ±nÄ±
    # 'success' (baÅŸarÄ±) mesajÄ± ile deÄŸiÅŸtir.
    st.sidebar.success("RAG Zinciri ve Model BaÅŸarÄ±yla YÃ¼klendi!")
    return rag_chain # HafÄ±zaya (cache) alÄ±nacak olan RAG zincirini dÃ¶ndÃ¼r.

# ==============================================================================
# WEB ARAYÃœZÃœ (STREAMLIT)
# AmaÃ§: KullanÄ±cÄ±nÄ±n gÃ¶receÄŸi ve etkileÅŸime gireceÄŸi arayÃ¼zÃ¼ Ã§izdirmek.
# ==============================================================================

# st.markdown: Ekrana metin (veya HTML) basar.
# 'unsafe_allow_html=True', <style> ve <h1> gibi HTML etiketlerini
# kullanmamÄ±za izin verir (BaÅŸlÄ±ÄŸÄ± kÃ¼Ã§Ã¼ltmek iÃ§in).
st.markdown("""
<style>
h1 {
    font-size: 2.0rem !important;
}
</style>
<h1>ğŸµ ÅarkÄ± SÃ¶zlerinden ÅarkÄ± Bulma Chatbot'u ğŸµ</h1>
""", unsafe_allow_html=True)

st.markdown("Merhaba, nasÄ±l yardÄ±mcÄ± olabilirim?")

# --- Ana Uygulama MantÄ±ÄŸÄ± ---
# Bu 'try-except' bloÄŸu, RAG zinciri yÃ¼klenirken veya Ã§alÄ±ÅŸÄ±rken
# (Ã¶rn: bozuk veritabanÄ±, yanlÄ±ÅŸ API anahtarÄ±) oluÅŸabilecek
# herhangi bir hatayÄ± yakalar ve uygulamanÄ±n Ã§Ã¶kmesini engeller.
try:
    # ADIM 1: RAG zincirini yÃ¼kle.
    # (Bu fonksiyon @st.cache_resource sayesinde sadece ilk seferde
    # gerÃ§ekten Ã§alÄ±ÅŸÄ±r, sonraki seferlerde hafÄ±zadan (cache) Ã§aÄŸrÄ±lÄ±r.)
    chain = load_rag_chain()
    
    # ADIM 2: KullanÄ±cÄ±dan soru almak iÃ§in bir metin giriÅŸ kutusu oluÅŸtur.
    user_question = st.text_input("Bulmak istediÄŸiniz ÅŸarkÄ± sÃ¶zÃ¼nÃ¼ yazÄ±n:")

    # ADIM 3: EÄŸer kullanÄ±cÄ± bir soru yazÄ±p 'Enter'a bastÄ±ysa...
    if user_question:
        # 'st.spinner', kullanÄ±cÄ±ya "bekle" mesajÄ± gÃ¶steren
        # dÃ¶nen bir animasyon baÅŸlatÄ±r.
        with st.spinner("ÅarkÄ± aranÄ±yor... ğŸ¶"):
            # RAG zincirini kullanÄ±cÄ±nÄ±n sorusuyla Ã§alÄ±ÅŸtÄ±r (invoke).
            cevap = chain.invoke(user_question) 
        
        # 'with st.spinner' bloÄŸu bittiÄŸinde (cevap alÄ±ndÄ±ÄŸÄ±nda)
        # animasyon otomatik olarak kaybolur.
        
        # ADIM 4: Gelen cevabÄ± kontrol et.
        # Bu, RAG prompt'umuzdaki ('Bu konuda bilgim yok.') kuralÄ±
        # yakalamak iÃ§in Ã§ok Ã¶nemli bir kullanÄ±cÄ± deneyimi (UX) adÄ±mÄ±dÄ±r.
        if "Bu konuda bilgim yok" in cevap:
            # BaÅŸarÄ±sÄ±zlÄ±k durumu: LLM bir ÅŸey bulamadÄ±.
            # st.error: KÄ±rmÄ±zÄ± bir hata kutusu gÃ¶sterir.
            st.error("ÃœzgÃ¼nÃ¼m bulamadÄ±m. FarklÄ± bir sÃ¶z dizisi deneyebilirsin.")
        else:
            # BaÅŸarÄ± durumu: LLM bir cevap buldu.
            # st.success: YeÅŸil bir baÅŸarÄ± kutusu gÃ¶sterir.
            st.success("Ä°ÅŸte buldum!") 
            # st.markdown: CevabÄ± kalÄ±n (**) olarak ekrana basar.
            st.markdown(f"**Cevap:** {cevap}")

except Exception as e:
    # 'load_rag_chain' veya 'chain.invoke' sÄ±rasÄ±nda bir hata oluÅŸursa
    # (Ã¶rn: veritabanÄ± bozuksa, API anahtarÄ± geÃ§ersizse),
    # bu blok Ã§alÄ±ÅŸÄ±r ve kullanÄ±cÄ±ya net bir hata mesajÄ± gÃ¶sterir.
    st.error(f"Bir hata oluÅŸtu: {e}")
    st.error("LÃ¼tfen 'chroma_db' klasÃ¶rÃ¼nÃ¼n doÄŸru yerde olduÄŸundan veya API anahtarÄ±nÄ±zÄ±n doÄŸru olduÄŸundan emin olun.")